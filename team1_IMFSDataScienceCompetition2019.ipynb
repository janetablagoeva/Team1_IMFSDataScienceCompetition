{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 IMFS Data Science Competition - Netflix for Bonds\n",
    "## Week 0 - Setting Up\n",
    "Welcome to the 2019 IMFS Competition!\n",
    "\n",
    "### What (The Challenge)\n",
    "**Your challenge is to build a recommendation system that matches similar bond based on the revealed preferences of an expert trader.**\n",
    "\n",
    "**Backstory:** In an alternative universe, Vanguard employs several robotic crewmembers (manufactured by the Tyrell corporation) to perform certain parts of the investment lifecycle. One of the best robots, TRACE_Y, is very good at picking complementary bonds for various portfolios with an inexplicable knack for predicting which bonds will perform best in a diversified portfolio. Nobody has been able to replicate her work.\n",
    "Unfortunately, not only is TRACE_Y very good at picking bonds, she is also very good at picking the winning lottery numbers and just retired to a beach shack in Key West, where she plans to go off grid. This leaves you, the portfolio manager, with no way to pick bonds unless you can recreate the mind of the robot!\n",
    "All you have is a historical set of bonds that TRACE_Y looked at and her assessment of the ISIN of the nearest bond. Not every bond is labelled and the characteristics of the bond change from one day to the next.\n",
    "You have no idea what bond characteristics TRACE_Y weighed most heavily, but know it must be one or more of the fields presented in the attached files. Thankfully, you took Coursera classes on machine learning and think you might have what it takes to replicate her mind.\n",
    "You need to act fast – you have been given until July 31st to come up with an algorithm that can predict the nearest ISIN for several bonds. If you correctly predict the “nearest” bond (or even identify a bond in the top 10), you are confident that you will be able to continue to operate.\n",
    "\n",
    "**A slightly more serious explanation of the problem:** We’ve taken several months of data on various bonds and created a secret algorithm that ranks bonds by their similarity. It is based on knowledge of characteristics of the bonds and somewhat resembles decisions made by true portfolio managers. However, this ground truth is artificial, intentionally made up to provide a positive Kaggle experience. So while some knowledge of bonds and their characteristics may help you, a deep knowledge of fixed income markets will not help. **You are not predicting the true nearest bond (if you believe you know what that is), but the nearest bond as predicted by our algorithm, which remains secret.**\n",
    "\n",
    "### Getting Started\n",
    "You must join Microsoft Teams to stay up-to-date with the latest announcements for this competition.  \n",
    "\n",
    "Please ensure that you have joined the [Teams channel](https://teams.microsoft.com/l/channel/19%3af5fff6bb2bbf4493ae2ce674cea3b0d6%40thread.skype/General?groupId=e045e8fc-51f3-4d91-bbdc-583ff955ef24&tenantId=d3a74ac8-efe4-4fe8-b707-b1bf8c6a25bd) (refer to welcome email from Steve Lawrence).  \n",
    "\n",
    "If located in Australia, where Teams has not been deployed yet in the Vanguard networks, make sure you sign up to the IMFS slack channel for this competition (refer to welcome email from Steve Lawrence).  \n",
    "\n",
    "You will be working out of Kaggle, a platform for data science related competitions, to build and run your recommendation models. Therefore, you must create a Kaggle account and share your Kaggle handle with us (via Teams or Slack).  \n",
    "\n",
    "While you will do all the work in Kaggle, we also provide the Github repository if you want to work on your own mahine, GitHub hosts the main repository for this competition with all materials you will be using in this challenge.\n",
    "\n",
    "Please remember you can always work in Kaggle and you do not need to clone the github repository locally.\n",
    "\n",
    "### Your teammates\n",
    "John Kraynak  \n",
    "Mahesh Thummati  \n",
    "Krunal Patel  \n",
    "Janeta Blagoeva  \n",
    "Hemant Sojitra    \n",
    "\n",
    "### Your timeline\n",
    "Week 0 (starting July 1st) – Setting Up/Onboarding + Meet your IMFS mentor  \n",
    "Week 1 (Starting July 8th) – Understand the challenge + Tutorial on exploring your dataset in your Kaggle Kernel  \n",
    "Week 2 (starting July 15th) – Tutorial on preprocessing dataset  \n",
    "Week 3 (starting July 22nd) – Tutorial on building classification models and tuning the parameters  \n",
    "Week 4 (Starting July 29th) – Tutorial on submitting your results for grading  \n",
    "Wednesday July 31st – Last date for submission of competition results  \n",
    "Friday August 2nd – Award ceremony and announcement of winning teams  \n",
    "\n",
    "\n",
    "### The Dataset\n",
    "One year of historical pricing and duration data for a portfolio of securities.\n",
    "\n",
    "### The Rules\n",
    "1. **Participation**:\n",
    "   1. You will work in small teams of four to five people.\n",
    "   2. IMFS data scientist mentors will be hosting \"competition office hours\" where you can ask questions. The mentors will NOT do the work for you.\n",
    "   3. Your team will be assigned a mentor to help. She/he will check on you once a week to track on your progress\n",
    "   \n",
    "2. **How to approach the problem**  \n",
    "Here’s a brief rundown of what you need to do:\n",
    "   1. Join the competition on Microsoft Teams [HERE](https://teams.microsoft.com/l/channel/19%3af5fff6bb2bbf4493ae2ce674cea3b0d6%40thread.skype/General?groupId=e045e8fc-51f3-4d91-bbdc-583ff955ef24&tenantId=d3a74ac8-efe4-4fe8-b707-b1bf8c6a25bd).  We are using Microsoft TEAMS as it is widely available at Vanguard (US and Malvern). For our participants in Australia, communicate via Slack using the IMFS slack channel for this competition (refer to Steve Lawrence’s welcome email). Please remember to submit your GitHub and Kaggle account handles.\n",
    "   2. Use the dataset provided to train your model. Your model must take as an input a ISIN and return the top 10 similar securities to the \"ISIN\" in the test set. \n",
    "   3. Follow along the tutorial in your assigned competition kernel in Kaggle. A Kaggle kernel is a cloud computational environment that enables reproducible and collaborative analysis.  For this competition, the kernels are setup as Jupyter notebooks. Jupyter notebooks consist of a sequence of cells, where each cell is formatted in either Markdown (for writing text) or in a programming language of your choice (for writing code). For more details on kernels, read [this article](https://www.kaggle.com/docs/kernels) by Kaggle.\n",
    "   4. When you are ready to submit your work, run your model for each of the 1000 test securities in the [\"***test_set.csv***\" dataset].  \n",
    "   5. At the end of your notebook, save your recommendations for each security, see the function generate_output below.\n",
    "   6. Post \"Done with project - yourteamname\" in the competition's \"**#Microsoft Teams**\" channel or the **Slack channel** if in Australia.\n",
    "   \n",
    "3. **Ground of truth and grading**  \n",
    "The ground truth consists of a set of bonds and their characteristics and labels for the ISIN of the nearest bond. Information on the characteristics of this nearest bond are provided in the training file. As mentioned in the backstory, this ground truth is artificially generated to provide for an interesting competition and may not reflect the truly optimal bond for a particular situation. A good algorithm will correctly predict the ISIN of the nearest bond as labelled in the training dataset.\n",
    "\n",
    "We will grade your result in the following way: for each target ISIN in the test set, as long as your top 10 predictions contain the NearestISIN of the target ISIN, you successfully recommend it, otherwise you don't.\n",
    "   1. You will get perfect score if your recommendations match the model perfectly. Otherwise, will you will not get the score, so your score will be the number of NearestISIN you predict correct/1000.\n",
    "   2. The winning teams will be determined based on:\n",
    "      1. First Place (Performance based)\n",
    "      2. Best coded (as judged by Chuqi Yang)\n",
    "      3. Most innovative solution (As judged by IMFS team)\n",
    "      4. Most engaged (As judged by IMFS interns)\n",
    "\n",
    "### Help Desk\n",
    "#### IMFS Data Science Blog for tips and tutorials\n",
    "Access blog in Microsoft Teams Blog Page [HERE](https://thevanguardgroup.sharepoint.com/sites/2019imfsdatasciencecompetition/_layouts/15/news.aspx)   \n",
    "Ask questions in Microsoft Teams Channel [HERE](https://teams.microsoft.com/l/channel/19%3af5fff6bb2bbf4493ae2ce674cea3b0d6%40thread.skype/General?groupId=e045e8fc-51f3-4d91-bbdc-583ff955ef24&tenantId=d3a74ac8-efe4-4fe8-b707-b1bf8c6a25bd)\n",
    "\n",
    "#### Mentorship\n",
    "Your IMFS mentor is Yuhan (Flora) Huang (yuhan_huang@vanguard.com)    \n",
    "\n",
    "Her office hours:\n",
    "1. Week 1: Mon-Fri :11AM-2:45PM EST\n",
    "2. Week 2: Mon-Fri :11AM-2:45PM EST\n",
    "3. Week 3: Mon-Fri :11AM-2:45PM EST\n",
    "4. Week 4: Mon-Fri :11AM-2:45PM EST\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - Import packages and Load the Data\n",
    "\n",
    "In this section, you will import the necessary modules and packages you may need to use in the competition and load the data. The definition of modules and packages is here: https://www.learnpython.org/en/Modules_and_Packages\n",
    "\n",
    "Please feel free to import and use any modules and packages you think may help you in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading train and test datasets\n",
    "train_raw = pd.DataFrame.from_csv('../input/dataset_final.csv')\n",
    "test_raw = pd.DataFrame.from_csv('../input/test_set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading validation dataset\n",
    "val_test = pd.DataFrame.from_csv('../input/dataset_valtest.csv')\n",
    "val_ans =pd.DataFrame.from_csv('../input/dataset_valans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first take a look at the trian_raw\n",
    "from the cells below, we can find out that in \"trian_raw\", we have ISIN with its features in a specific day. There are 1397138 number of rows in the train_raw. However, some of them are not labeled, which we will use them in prediction rather than in training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see there are 1237625 rows has NearestISIN, they can be used in supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw[~train_raw['NearestISIN'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attributes we have\n",
    "train_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the test_raw\n",
    "from the cells below, we can find out that in \"test_raw\", for each ISIN in one day, we have 10 same rows, and the \"Your Prediction\" is None, you should fill the top 10 most similar bonds ISIN to replace these None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_raw.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_raw.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Understand Challenge\n",
    "before preprocessing and training the model, we had better to understand the challenge first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_ISIN = len(train_raw.ISIN.unique())\n",
    "print(total_ISIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we directly train Machine Learning given the data we have?\n",
    "\n",
    "As we can see from above, there are totally 6739 ISIN. If we only give the model the feature of a ISIN and ask the model to predict the most simialr bond of this ISIN, which means we have to train one model which has 6739 possible classes in prediction or trian 6739 models; neither of them are the optimal choice for us considering the time limitation, data sparsity and imbalanced data. Of course, you can try these two ways if time and resource allow, but in this tutorial we will directly skip this experimental stage and propose a way of preporcessing data to transform the challenge into a different machine learning problem.\n",
    "\n",
    "The way this tutorial will use this(you can use your method or change this method for sure): instead of using dataframe we have now, we use the absolute difference between two ISIN feature data to be the new feature, the label will be 1 or 0, 1 represents the two ISIN are the NearestISIN, 0 represents not. By using this data, we can train a binary classfication model!\n",
    "\n",
    "In the prediction stage, we will find out the how similar two bonds are by entering the absolute difference of feautres between two ISINs and then find out the top 10 simialr bonds for each ISIN. \n",
    "\n",
    "In the rest of this section, we will guide you through how to prepare and preproces for the training and testing data for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Preprocessing\n",
    "\n",
    "From the discussion above, for trainging set we will change each row of the dataset to the difference in features between two rows in the original dataframe. \n",
    "\n",
    "For testing set, we will change each row of the dataframe to the difference in features between target ISIN and one of the other ISIN in the trainingset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_train(df):\n",
    "    #Create index mapping\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    ## in this compeition, we only use random 10 days data to give you an example,\n",
    "    ## you can use more data if needed.\n",
    "    index_dic = {}\n",
    "    random_ten_days = random.sample(list(set(df['date'])),10)\n",
    "    df_indate = df[df['date'].isin(random_ten_days)]\n",
    "    df_indate = df_indate.reset_index(drop=True)\n",
    "    date_column, isin_column = df_indate['date'], df_indate['ISIN']\n",
    "    for i in range(len(date_column)):\n",
    "        index_dic[(date_column[i], isin_column[i])] = i\n",
    "    \n",
    "    #type of columns\n",
    "    IsCategorical = df_indate.dtypes == object\n",
    "    categorical_columns = [x for x in df_indate.columns[IsCategorical].tolist() if x not in ['date','ISIN','NearestISIN']]\n",
    "    numerical_columns = [x for x in df_indate.columns[~IsCategorical] if x not in ['date value','Keep']]\n",
    "    \n",
    "    #subset dataframe\n",
    "    categorical_df = df_indate[categorical_columns]\n",
    "    numerical_df = df_indate[numerical_columns]\n",
    "    \n",
    "    #nearestISIN dataframe (label 1)\n",
    "    numerical_sim = []\n",
    "    categorical_sim = []\n",
    "    date_sim = []\n",
    "    isin_sim = []\n",
    "    nearest_isin_sim = []\n",
    "    for row_index, row in df_indate.iterrows():\n",
    "        date, isin, nearest_isin = row['date'], row['ISIN'], row['NearestISIN']\n",
    "        if type(nearest_isin) == str:\n",
    "            nearest_isin_index = index_dic[(date, nearest_isin)]  \n",
    "            numerical_sim.append(abs(numerical_df.iloc[row_index]-numerical_df.iloc[nearest_isin_index]))\n",
    "            categorical_sim.append(categorical_df.iloc[row_index]==categorical_df.iloc[nearest_isin_index])\n",
    "            date_sim.append(date)\n",
    "            isin_sim.append(isin)\n",
    "            nearest_isin_sim.append(nearest_isin)\n",
    "        else:\n",
    "            continue\n",
    "    sim_df = pd.concat([pd.DataFrame(np.column_stack([date_sim,isin_sim,nearest_isin_sim]), columns=['date', 'ISIN', 'NearestISIN']),\n",
    "                        pd.DataFrame(numerical_sim), pd.DataFrame(categorical_sim)], axis=1)\n",
    "    sim_df['Response'] = 1\n",
    "    \n",
    "    #random not nearestISIN dataframe (label 0)\n",
    "    numerical_diff = []\n",
    "    categorical_diff = []\n",
    "    date_diff = []\n",
    "    isin_diff = []\n",
    "    nearest_isin_diff = []\n",
    "    indexes = [x for x in range(df_indate.shape[0])]\n",
    "    for row_index, row in df_indate.iterrows():\n",
    "        date, isin, nearest_isin = row['date'], row['ISIN'], row['NearestISIN']\n",
    "        if type(nearest_isin) == str:\n",
    "            nearest_isin_index = index_dic[(date, nearest_isin)]\n",
    "            \n",
    "            random_index = random.choice(indexes)\n",
    "            while  random_index ==row_index or random_index==nearest_isin_index:\n",
    "                random_index = random.choice(indexes)\n",
    "            numerical_diff.append(abs(numerical_df.iloc[row_index]-numerical_df.iloc[random_index]))\n",
    "            categorical_diff.append(categorical_df.iloc[row_index]==categorical_df.iloc[random_index])\n",
    "            date_diff.append(date)\n",
    "            isin_diff.append(isin)\n",
    "            nearest_isin_diff.append(nearest_isin)\n",
    "        else:\n",
    "            continue\n",
    "    diff_df = pd.concat([pd.DataFrame(np.column_stack([date_diff,isin_diff,nearest_isin_diff]), columns=['date', 'ISIN', 'NearestISIN']),\n",
    "                        pd.DataFrame(numerical_diff), pd.DataFrame(categorical_diff)], axis=1)\n",
    "    diff_df['Response'] = 0\n",
    "    \n",
    "    #output df\n",
    "    output_df = sim_df.append(diff_df.reset_index(drop=True))\n",
    "    output_df = output_df.reset_index(drop=True)\n",
    "    end = time.time()\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = clean_train(train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we are going to preprocess the test set, in theory, for each test ISIN, we have 6739 rows to generate in test set, so the following step will take a comparatively long time(may longer than 20 mins) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_test(train_raw, test_raw):\n",
    "    \n",
    "    #Create index mapping\n",
    "    # Find last date, as this is the test sample\n",
    "    last_date = test_raw.date[0]\n",
    "    \n",
    "    # Subset the dataframe for those in the last date, and reset index\n",
    "    df_indate = train_raw[train_raw['date']==last_date]\n",
    "    df_indate = df_indate.reset_index(drop=True)\n",
    "    \n",
    "    # Take dates and columns to be used later for dictionary\n",
    "    date_column, isin_column = df_indate['date'], df_indate['ISIN']\n",
    "    \n",
    "    # As in previous function\n",
    "    IsCategorical = df_indate.dtypes == object\n",
    "    categorical_columns = [x for x in df_indate.columns[IsCategorical].tolist() if x not in ['date','ISIN','NearestISIN']]\n",
    "    numerical_columns = [x for x in df_indate.columns[~IsCategorical] if x not in ['date value','Keep']]\n",
    "    \n",
    "    # subset dataframe\n",
    "    categorical_df = df_indate[categorical_columns]\n",
    "    numerical_df = df_indate[numerical_columns]\n",
    "    \n",
    "    # Sample categorical for numerical categories\n",
    "    categorical_mat = np.zeros(np.shape(categorical_df))\n",
    "\n",
    "    # Convert categories into numbers\n",
    "    for k, column in enumerate(categorical_df):\n",
    "        category_set = categorical_df[column].unique()\n",
    "        num_val = np.zeros(len(category_set))\n",
    "        for j, cat in enumerate(category_set):\n",
    "            categorical_mat[categorical_df[column]==cat,k] = j + 1\n",
    "\n",
    "    # Create overall matrix\n",
    "    feature_mat = np.hstack((categorical_mat, numerical_df.values))\n",
    "    \n",
    "    # Create array of ISINs to match to feature_mat\n",
    "    all_isin = df_indate['ISIN'].values\n",
    "    \n",
    "    # Test ISIN is all ISINs that we need to solve for\n",
    "    test_ISIN = set(test_raw.ISIN.unique())\n",
    "    \n",
    "    # List of all these isins\n",
    "    test_indate_df = df_indate[df_indate.ISIN.isin(test_ISIN)]\n",
    "    test_isin_list = test_indate_df['ISIN'].values\n",
    "   \n",
    "    # Set sample size differently if testing for a sub sample - otherwise leave\n",
    "    sample = len(test_isin_list)\n",
    "\n",
    "    # -1 since 1 - total isins for each possible isin\n",
    "    step_isin = np.shape(feature_mat)[0]-1\n",
    "        \n",
    "    # Preallocate matrix of difference [npossibleIsin x nFeatures x nTestIsin]\n",
    "    diff_mat = np.zeros((step_isin,np.shape(feature_mat)[1], len(test_isin_list[:sample])))\n",
    "    isin_target_mat = (step_isin * len(test_isin_list[:sample])) * [None]\n",
    "    isin_test_mat = (step_isin * len(test_isin_list[:sample])) * [None]\n",
    "    \n",
    "    # Loop through each test isin and create matrix of differences\n",
    "    for k, isin in enumerate(test_isin_list[:sample]):\n",
    "        isin_idx = all_isin == isin\n",
    "        diff_mat[:,:,k] = abs(feature_mat[~isin_idx,:] - feature_mat[isin_idx,:])\n",
    "        isin_target_mat[step_isin*k:(step_isin*k)+step_isin] = step_isin * [isin]\n",
    "        isin_test_mat[step_isin*k:(step_isin*k)+step_isin] = isin_column[~isin_idx]       \n",
    "        \n",
    "    # Reshape into 2-D matrix by stacking third dimension\n",
    "    diff_mat_2d = np.reshape(np.transpose(diff_mat, (2,0,1)),(sample * step_isin, np.shape(feature_mat)[1]), order = 'C')\n",
    "    \n",
    "    # Date is the same for all\n",
    "    date = np.shape(diff_mat_2d)[0] * [last_date]\n",
    "    \n",
    "    # For all categoricals turn to true if match, false otherwise\n",
    "    categorical_bool = diff_mat_2d[:,:len(categorical_df.columns)] == 0\n",
    "    \n",
    "    # Take numerical matrix for appropriate part\n",
    "    num_mat = diff_mat_2d[:,len(categorical_df.columns):]\n",
    "    \n",
    "    # Combine into one output dataframe for testing\n",
    "    diff_df = pd.concat([pd.DataFrame(np.column_stack([date,isin_target_mat,isin_test_mat]), columns=['date', 'ISIN','ThisISIN']),\n",
    "                        pd.DataFrame(num_mat, columns = numerical_columns), pd.DataFrame(categorical_bool, columns = categorical_columns)], axis=1)\n",
    "    \n",
    "    # Reset Index\n",
    "    output_df = diff_df.reset_index(drop=True)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = clean_test(train_raw, test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split X and y\n",
    "after getting train_df and test_df, we need to split dataset into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain = train_df.drop(['date', 'ISIN', 'NearestISIN', 'level_0', 'index', 'Response'], axis = 1)\n",
    "ytrain = train_df['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = test_df.drop(['date', 'ISIN', 'ThisISIN'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Fit data into a model\n",
    "here we choose random forest model to give you an idea how we feed the data into a model, here we demonstrate you the model random forest with many techniques/tricks to tune the model in the package sklearn; but random forest is only one of the models in the machine universe, there are many different models and different methods you can use to tune the model, enjoy it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomforest_proba(xtrain,xtest,ytrain):\n",
    "    categorical_features = xtrain.dtypes == \"object\"\n",
    "    numerical_features = ~categorical_features\n",
    "    \n",
    "    preprocess_scale = make_column_transformer(\n",
    "    (numerical_features, make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())),\n",
    "    (categorical_features, make_pipeline(SimpleImputer(fill_value='missing', strategy='constant'), \n",
    "                                         OneHotEncoder(handle_unknown=\"ignore\"))))\n",
    "    \n",
    "    randomforest_scale = Pipeline([('preprocess', preprocess_scale), \n",
    "                 ('clf', RandomForestClassifier(warm_start=True,n_jobs=-1))])\n",
    "    \n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    random_grid = {'clf__max_depth': max_depth}\n",
    "    grid_rf = GridSearchCV(randomforest_scale, random_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "    print(\"===Start of grid search===\")\n",
    "    grid_rf.fit(xtrain, ytrain)\n",
    "    print(\"===End of grid search===\")\n",
    "  \n",
    "    cross_score = grid_rf.best_score_\n",
    "    y_preds = grid_rf.predict_proba(xtest)[:,1]\n",
    "    print(\"Cross-validated training score is:\")\n",
    "    print(cross_score)\n",
    "    \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_pred_proba_d = randomforest_proba(x_train,x_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Test on validation test set\n",
    "Since we are transfer learning a binary classification problem to a clustering problem, we provide you a validation set with answer to give you a sense how well our model will be on the real test dataset. The accuracy score can be different from the cross-validation score you just get, so you might need to continue improving your model to make sure it is predicting the 10 Nearest ISINs well too! :D\n",
    "\n",
    "Please Note that the result in validation set is not your score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df = clean_test(train_raw, val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = val_df.drop(['date', 'ISIN', 'ThisISIN', 'level_0', 'index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_preds = randomforest_proba(x_train,x_val,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transfer_test(test_df, val_test, y_preds, y_true):\n",
    "    #create dataframe\n",
    "    test_df['Class_Prob'] = y_preds\n",
    "    sel_ISIN = list(test_df.ISIN.unique())\n",
    "    nearest_10ISIN = {}\n",
    "    for isin in sel_ISIN:\n",
    "        temp = test_df[test_df.ISIN == isin]\n",
    "        temp_sort = temp.sort_values(['Class_Prob'],ascending=False).head(10)\n",
    "        nearest_10ISIN[isin] = temp_sort['ThisISIN'].tolist()\n",
    "    for idx in range(val_test.shape[0]):\n",
    "        isin = val_test.loc[idx, 'ISIN']\n",
    "        val_test.loc[idx,'Your Prediction'] = nearest_10ISIN[isin][0]\n",
    "        nearest_10ISIN[isin].pop(0)\n",
    "\n",
    "    correct = 0\n",
    "    for idx in range(y_true.shape[0]):\n",
    "        pred_list = val_test[val_test['ISIN']==y_true.loc[idx,'ISIN']]['Your Prediction'].tolist()\n",
    "        if y_true.loc[idx,'NearestISIN'] in pred_list:\n",
    "            correct+=1\n",
    "    \n",
    "    print(\"Test accuracy score is:\")\n",
    "    print(correct/len(sel_ISIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transfer_test(val_df, val_test, val_preds, val_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Generate results\n",
    "\n",
    "\n",
    "From the function above, we generate the probability of each pairs are NearestISINs, so for each ISIN in the test set, we want to find out the top 10 similar bonds\n",
    "After finding out the top 10 similar bonds we want to save them into test_raw, and we just need to save the test_raw into a csv file with name 'TeamX_output.csv'(X means team number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(test_df, test_raw, y_preds, filename):\n",
    "    #create dataframe\n",
    "    test_df['Class_Prob'] = y_preds\n",
    "    sel_ISIN = list(test_df.ISIN.unique())\n",
    "    nearest_10ISIN = {}\n",
    "    for isin in sel_ISIN:\n",
    "        temp = test_df[test_df.ISIN == isin]\n",
    "        temp_sort = temp.sort_values(['Class_Prob'],ascending=False).head(10)\n",
    "        nearest_10ISIN[isin] = temp_sort['ThisISIN'].tolist()\n",
    "    for idx in range(test_raw.shape[0]):\n",
    "        isin = test_raw.loc[idx, 'ISIN']\n",
    "        test_raw.loc[idx,'Your Prediction'] = nearest_10ISIN[isin][0]\n",
    "        nearest_10ISIN[isin].pop(0)\n",
    "    import os\n",
    "    print(os.getcwd())\n",
    "    test_raw.to_csv(filename)\n",
    "    print(\"Output written to file \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_output(test_df, test_raw, rf_pred_proba_d, 'TeamX_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"TeamX_output.csv\"> Download File </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
